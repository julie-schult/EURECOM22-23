{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d824fdc6-1e6b-474f-bd24-78cff77ce684",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup\n",
    "You should use the base Python installation that is installed on EURECOM Linux computer laboratory machines.  You can also use your own machines, but you'll need to check that your installation is up to date and has the same library versions as EURECOM machines.  The lab is untested for Windows environments and may require the installation of ffmpeg libraries in addition to those listed below.  The lab will allow you to listen to some audio files.  **Please be careful to adjust the volume to a safe level before listening to them.**\n",
    "\n",
    "You should work through the laboratory executing each cell as you go by clicking on the cell and pressing <tt>shift+enter</tt>.  Do not run everything in one go.\n",
    "\n",
    "We will need the following libraries and functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a0797-91bf-426d-a4c2-bc016b7dd35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import IPython.display\n",
    "import random\n",
    "import scipy\n",
    "import librosa\n",
    "import librosa.display\n",
    "import lpc\n",
    "from sklearn.cluster import KMeans\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78eec82-a493-4779-9109-76c1268816b5",
   "metadata": {},
   "source": [
    "# Spectrogram analysis\n",
    "The objective of the exercises in this section are to improve your intuitive understanding of frame blocking, the discrete Fourier transform, the short-term Fourier transform and of spectro-temporal decopomositions such as the <i>spectrogram</i>.\n",
    "\n",
    "We start by defining some default parameters used for spectral analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00ae0f4-6062-4eb7-88b9-33f6cabd01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsr=8000         # defaut sampling rate\n",
    "dn_fft=256       # default Fourier transform size\n",
    "dhop_length=128  # default hop_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b40177-a5dc-4087-9bc5-5b61460a6e5f",
   "metadata": {},
   "source": [
    "## Chirp signals\n",
    "You will generate three different types of chirp signal. Two have increasing frequency, the other decreasing frequency. Two are expononential chirps, the other linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd490fb3-bdb8-48c6-ade5-271dd064352a",
   "metadata": {},
   "source": [
    "### Positive exponential chirp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d3945-f2fe-4d45-af79-8a1ca51f780d",
   "metadata": {},
   "source": [
    "We'll create a 1-second exponential chirp signal starting at 1 Hz and ending at 4 kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf1130-a5dd-4586-b4f0-42e5317466fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_chirp = librosa.chirp(fmin=1, fmax=4000, sr=dsr, duration=1)\n",
    "\n",
    "# plot the time waveform\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "librosa.display.waveshow(exponential_chirp,sr=dsr)\n",
    "ax.set(title='Exponential chirp waveform')\n",
    "ax.set_xlim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b6078-4390-41a8-9c7a-9140e53de1cf",
   "metadata": {},
   "source": [
    "Let's listen to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a50d7c-0a11-48da-8370-8bef2199d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=exponential_chirp, rate=dsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1b1b7-c307-441f-b8b8-a6bdbfe6183f",
   "metadata": {},
   "source": [
    "Explain why the sound seems to last less than 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e1d0e-5def-45fc-8d82-59066f3eb71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spectrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "S_exponential = np.abs(librosa.stft(exponential_chirp, n_fft=dn_fft, \n",
    "                                    hop_length=dhop_length, center=False))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S_exponential, ref=np.max), \n",
    "                         n_fft=dn_fft, hop_length=dhop_length,\n",
    "                         sr=dsr, x_axis='time', y_axis='linear', \n",
    "                         ax=ax, cmap='jet')\n",
    "ax.set(title='Exponential chirp spectrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658509e1-e861-4980-855b-7abd0dc7a873",
   "metadata": {},
   "source": [
    "Explain why you see such wide bands or smearing for the chirp at higher frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc4bdae-3b71-4d07-bad3-12f991263f0d",
   "metadata": {},
   "source": [
    "### Negative exponential chirp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e229a2-a097-4768-934a-182296e34537",
   "metadata": {},
   "source": [
    "Now we'll do exactly the same, but for a chirp signal decreasing from 4 kHz down to 1 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b186322-d70f-4ca6-916c-36e3d3648972",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_exponential_chirp = librosa.chirp(\n",
    "    fmin=4000, fmax=1, sr=dsr, duration=1)\n",
    "\n",
    "# plot the time waveform\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "librosa.display.waveshow(negative_exponential_chirp,sr=dsr)\n",
    "ax.set(title='Negative exponential chirp waveform')\n",
    "ax.set_xlim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4de03c-b9e0-43d3-8845-4d6ea870852a",
   "metadata": {},
   "source": [
    "Listen to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2a955-2b65-4c9c-9ca2-9832cfd8a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=negative_exponential_chirp, rate=dsr) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8185bb7-1e6f-47f7-90dd-2dcad7dd74e9",
   "metadata": {},
   "source": [
    "**Before** plotting it, think about what you would expect to see in the spectrogram.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76407072-f6bc-4868-9fd9-874cdd444755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spectrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "S_negative_exponential = np.abs(librosa.stft(\n",
    "    negative_exponential_chirp, n_fft=dn_fft, hop_length=dhop_length,\n",
    "    center=False))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(\n",
    "    S_negative_exponential, ref=np.max), n_fft=dn_fft, \n",
    "    hop_length=dhop_length, sr=dsr, x_axis='time', \n",
    "    y_axis='linear', ax=ax, cmap='jet')\n",
    "ax.set(title='Negative exponential chirp spectrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fea91-e35f-408a-aa93-fa5c1de7251d",
   "metadata": {},
   "source": [
    " Describe the correspondance between the chirp characteristics, the timewave form and the spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064da7ec-ba28-4468-b2dc-ed8752bb28bf",
   "metadata": {},
   "source": [
    "### Positive linear chirp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfeaeea-a892-4ddb-ae50-7069724d07e9",
   "metadata": {},
   "source": [
    "Now exactly the same, but for a positive, linear chirp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc25e39-9d3d-44ae-afb1-8e4173ceb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_chirp = librosa.chirp(fmin=1, fmax=4000, sr=dsr,\n",
    "                             duration=1, linear=True)\n",
    "# plot the time waveform\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "librosa.display.waveshow(linear_chirp,sr=dsr)\n",
    "ax.set(title='Linear chirp waveform')\n",
    "ax.set_xlim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2be30-d7bb-49df-9bba-dbc374c1f07e",
   "metadata": {},
   "source": [
    "Listen to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad74499-5e4f-4165-a6a6-876daac988fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=linear_chirp, rate=dsr) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7bab2a-2f01-4bf6-84f4-c02c9f0c75c7",
   "metadata": {},
   "source": [
    "Explain why the sound now seems to last the full 1 second, or at least longer than in the first two examples.\n",
    "\n",
    "Again, **before** plotting it, think about what you would expect to see in the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee465f8-c6c3-4231-ae02-917841e29967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spectrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "S_linear = np.abs(librosa.stft(linear_chirp, n_fft=dn_fft, \n",
    "                               hop_length=dhop_length, center=False))\n",
    "librosa.display.specshow(\n",
    "    librosa.amplitude_to_db(S_linear, ref=np.max), n_fft=dn_fft, \n",
    "    hop_length=dhop_length, sr=dsr, x_axis='time', \n",
    "    y_axis='linear', ax=ax, cmap='jet')\n",
    "ax.set(title='Linear chirp spectrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3e09e4-943c-4b31-a896-db47fd048043",
   "metadata": {},
   "source": [
    "Describe the sectrogram, the differences between the spectrograms of each chirp signal and why there is less smearing across frequency for the linear chirp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e0c53d-3ea2-4295-b215-28d2afc209b2",
   "metadata": {},
   "source": [
    "### A whistle\n",
    "We will now look at the spectrogram analysis of a real signal, in this case a whistle sound at a more-or-less constant frequency.  Listen to the signal and then try to imagine what would be the corresponding spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0033552-38ae-4ccb-a7a1-e4000b226107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whistle sound, listen to it and plot the waveform\n",
    "whistle,dsr = librosa.load('siffle.wav', sr=dsr)\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "librosa.display.waveshow(whistle,sr=dsr)\n",
    "ax.set(title='Whistle waveform')\n",
    "ax.set_xlim([0, len(whistle)/dsr])\n",
    "IPython.display.Audio(data=whistle, rate=dsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e6bf19-fe62-43ec-bfc7-d875a28557c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spectrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "S_whistle = np.abs(librosa.stft(\n",
    "    whistle, n_fft=dn_fft, hop_length=dhop_length,center=False))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S_whistle, ref=np.max), \n",
    "                         n_fft=dn_fft, hop_length=dhop_length, sr=dsr, \n",
    "                         x_axis='time', y_axis='linear', ax=ax, cmap='jet')\n",
    "ax.set(title='Whistle spectrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12a4ef-a050-4134-9f82-f6db207ae2bd",
   "metadata": {},
   "source": [
    "What is the approximate frequency of the whistle?  To what corresponds the second visible component? While it is a little difficult to see, describe the correspondance between the amplitude in the waveform and the colour variation in the spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6e179-d5fb-4213-beaf-d71cf150b463",
   "metadata": {},
   "source": [
    "## A speech utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80337535-f7d2-4e79-9e65-04458b4c770b",
   "metadata": {},
   "source": [
    "Finally, we'll look at a real speech signal.  It's a recording of the French word '*assassiner*'. Like before, we'll plot the waveform and the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b348842-5d9f-4605-979d-a0a45154df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the speech signal and plot the waveform\n",
    "assassine,dsr = librosa.load('assassine.wav', sr=dsr)\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "librosa.display.waveshow(assassine,sr=dsr)\n",
    "ax.set(title='Speech waveform')\n",
    "ax.set_xlim([0, len(assassine)/dsr])\n",
    "IPython.display.Audio(data=assassine, rate=dsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3006f2c-7bed-48f8-8078-7f08c4a7f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spectrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "S_assassine = np.abs(librosa.stft(\n",
    "    assassine, n_fft=dn_fft, hop_length=dhop_length, center=False))\n",
    "librosa.display.specshow(\n",
    "    librosa.amplitude_to_db(S_assassine, ref=np.max), n_fft=dn_fft, \n",
    "    hop_length=dhop_length, sr=dsr, x_axis='time', y_axis='linear',\n",
    "    ax=ax, cmap='jet')\n",
    "ax.set(title='Speech spectrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef3b37-5013-4c69-8fb1-d6cb2f500372",
   "metadata": {},
   "source": [
    "Describe the spectrogram and the correspondance to the waveform. Identity the intervals of voiced and unvoiced speech. What are the red, horizontal striations? Estimate the fundamental frequency for the vowel sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2346b5f-7c32-4477-8361-c5981e4aadfa",
   "metadata": {},
   "source": [
    "## Narrowband and wideband speech spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa662a-d9fc-4539-a6f9-ef13c603f45a",
   "metadata": {},
   "source": [
    "We will produce two different spectrograms for the same example we saw in your lectures, the <tt>9-1-9-6-9-5-1</tt> number sequence pronounced by a female, American speaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e71f8-b2d5-4cd2-b7e2-817860611786",
   "metadata": {},
   "outputs": [],
   "source": [
    "numSeq, sr = librosa.load('FAK_9196951A.wav', sr=dsr)\n",
    "IPython.display.Audio(data=numSeq, rate=dsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d4167-f6a0-42e1-a512-4042ab9da1f2",
   "metadata": {},
   "source": [
    "First, we'll plot a narrowband spectrogram, then a wideband spectrogram. Note that each is produced with a different DFT size <tt>n_fft</tt> and different hop length <tt>hop_length</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50ccb9-da7c-4a8f-978a-3876b60831f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, sharex=True, figsize=(15, 10))\n",
    "librosa.display.waveshow(numSeq, sr=dsr, ax=ax[0], x_axis=None)\n",
    "ax[0].set_xlim([0, len(numSeq)/dsr])\n",
    "ax[0].set(title='9-1-9-6-9-5-1 time waveform');\n",
    "\n",
    "n_fft=64\n",
    "hop_length=64\n",
    "S_numSeq = np.abs(librosa.stft(numSeq, n_fft=n_fft, \n",
    "                               hop_length=hop_length))\n",
    "librosa.display.specshow(\n",
    "    librosa.amplitude_to_db(S_numSeq, ref=np.max), n_fft=n_fft,\n",
    "    hop_length=hop_length, sr=dsr, x_axis='time', \n",
    "    y_axis='linear', ax=ax[1], cmap='jet')\n",
    "ax[1].set(title='Wideband spectrogram', xlabel=None);\n",
    "\n",
    "n_fft=256\n",
    "hop_length=64\n",
    "S_numSeq = np.abs(librosa.stft(numSeq, n_fft=n_fft, \n",
    "                               hop_length=hop_length))\n",
    "librosa.display.specshow(\n",
    "    librosa.amplitude_to_db(S_numSeq, ref=np.max), n_fft=n_fft, \n",
    "    hop_length=hop_length, sr=dsr, x_axis='time', \n",
    "    y_axis='linear', ax=ax[2], cmap='jet')\n",
    "ax[2].set(title='Narrowband spectrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21474d98-09b9-49c8-bc8e-6ce05030b9b0",
   "metadata": {},
   "source": [
    "Going beyond the differences in terms of <tt>n_fft</tt> and <tt>hop_length</tt>, compare and contrast the two spectrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c078c07f-3c0d-47a6-b012-29360f398613",
   "metadata": {},
   "source": [
    "## Speech bandwidth and quality\n",
    "\n",
    "Now we'll use spectrogrms to hep study the relationship between bandwidth and perceived speech quality.  We're going to listen to a high bandwidth utterance, look at the relative energy in the high band, and then listen to progressively downsampled versions of the same utterance.  We'll start with an utterance sampled at 22 kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439fe67-db19-4788-a988-29c4387ca7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recording extracted from the Voice Cloning Toolkit (VCTK) Database\n",
    "# https://datashare.ed.ac.uk/handle/10283/3443\n",
    "# used under the following licenses\n",
    "# https://datashare.ed.ac.uk/bitstream/handle/10283/3443/license_text\n",
    "y_fb, sr = librosa.load('p313_023.wav')\n",
    "IPython.display.Audio(data=y_fb, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33dab95-c081-4475-b354-882e9f659349",
   "metadata": {},
   "source": [
    "Plot the time waveform and the spectrogam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3739c-cbef-4090-ac5e-85301b6095d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, sharex=True, figsize=(15, 6))\n",
    "librosa.display.waveshow(y_fb, sr=sr, ax=ax[0])\n",
    "ax[0].set_xlim([0, len(y_fb)/sr])\n",
    "ax[0].set(title='22 kHz', xlabel=None);\n",
    "\n",
    "S_hf = np.abs(librosa.stft(y_fb, n_fft=dn_fft, hop_length=dhop_length))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S_hf, ref=np.max), \n",
    "                         n_fft=dn_fft, hop_length=dhop_length,\n",
    "                         sr=sr, x_axis='time', y_axis='linear', \n",
    "                         ax=ax[1], cmap='jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dfc593-4f29-45b2-84eb-8b2d10c4881c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Notice that there are components across the full spectrum.  We will now repeat the above with the same utterance downsampled to an 8 kHz sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181fdad-5932-4cc8-bd0b-d5a1309798d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sr=8000\n",
    "y_nb = librosa.resample(y_fb, orig_sr=sr, target_sr=target_sr)\n",
    "IPython.display.Audio(data=y_nb, rate=target_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ab51e-e682-40d9-ab59-edd3e02f43a2",
   "metadata": {},
   "source": [
    "Describe the differences in terms of perceived quality.\n",
    "\n",
    "Plot the time waveform and the spectrogam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7be1bb-43c4-458a-9a0e-0a44fbcd2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, sharex=True, figsize=(15, 6))\n",
    "librosa.display.waveshow(y_nb, sr=target_sr, ax=ax[0])\n",
    "ax[0].set_xlim([0, len(y_nb)/target_sr])\n",
    "ax[0].set(title='8 kHz', xlabel=None);\n",
    "\n",
    "S_nb = np.abs(librosa.stft(y_nb, n_fft=dn_fft, hop_length=dhop_length))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S_nb, ref=np.max), \n",
    "                         n_fft=dn_fft, hop_length=dhop_length,\n",
    "                         sr=target_sr, x_axis='time', y_axis='linear',\n",
    "                         ax=ax[1], cmap='jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef93a3-4b1f-4b10-8421-1b3cf59c1fa2",
   "metadata": {},
   "source": [
    "Finally, we'll downsample to a sampling rate of 4 kHz and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfe3e5-4fd9-46d2-a432-5a1aa853e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sr=4000\n",
    "y_nb = librosa.resample(y_fb, orig_sr=sr, target_sr=target_sr)\n",
    "IPython.display.Audio(data=y_nb, rate=target_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc658a-8fbd-47cf-883d-5e3084c9e990",
   "metadata": {},
   "source": [
    "Plot the time waveform and the spectrogam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a98874-9ff7-42ce-a6a1-4c8470fd6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, sharex=True, figsize=(15, 6))\n",
    "librosa.display.waveshow(y_nb, sr=target_sr, ax=ax[0])\n",
    "ax[0].set_xlim([0, len(y_nb)/target_sr])\n",
    "ax[0].set(title='4 kHz', xlabel=None);\n",
    "\n",
    "S_nb = np.abs(librosa.stft(y_nb, n_fft=dn_fft, hop_length=dhop_length))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S_nb, ref=np.max), \n",
    "                         n_fft=dn_fft, hop_length=dhop_length,\n",
    "                         sr=target_sr, x_axis='time', y_axis='linear',\n",
    "                         ax=ax[1], cmap='jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d53db6-d088-48b9-a90b-e3389e7b46aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Notice the substantial degradation in quality.  Describe your observations and explain why the degradation is so much more significant at 4 kHz than at 8 kHz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b3a8e-6b4f-4fa1-8774-afd1d9254b3f",
   "metadata": {},
   "source": [
    "# Voiced/unvoiced speech and the fundamental frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0544536-f856-477c-9ce1-831dfa0e5c52",
   "metadata": {},
   "source": [
    "We will now look at the spectral attributes of voiced and unvoiced speech by looking at the spectra of specific intervals of speech.  Each spectrum is extracted from a single frame and is one column of the spectrogram. We'll revert to the same, short recording used earlier, namely that of the French word 'assassiner', the spectrogram of which we will plot again for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b264d-4450-4087-bc20-428b60e94d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spectrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "S_assassine = np.abs(\n",
    "    librosa.stft(assassine, center=False, n_fft=dn_fft,\n",
    "                 hop_length=dhop_length))\n",
    "S_assassine_db = librosa.amplitude_to_db(S_assassine, ref=np.max)   \n",
    "# center=False, otherwise the \n",
    "# number of frames is not equal\n",
    "# to the number of times produced \n",
    "# by librosa.util.frame\n",
    "\n",
    "librosa.display.specshow(S_assassine_db, \n",
    "                         n_fft=dn_fft, hop_length=dhop_length,\n",
    "                         sr=dsr, x_axis='time', y_axis='linear', \n",
    "                         ax=ax, cmap='jet')\n",
    "ax.set(title='Speech spectrogram', xlabel=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e252f26-3cf2-4c70-a0a7-00d51f447849",
   "metadata": {},
   "source": [
    "Let's select a frame of speech in the vicinity of some particular time instant. You can choose a different time instant according to whether you want to inspect an interval of voiced or unvoiced speech. There is no  error checking, so be sure to choose an instant between 0 and 1.2 seconds. To start with, we'll chose a instant at random, but you should set it yourself according to whether you want to look at voiced or unvoiced speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a354d87-02e7-4b84-af8f-8b87fddc2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = random.uniform(0, 1)*len(assassine)/dsr \n",
    "# uncomment and change the following to whatever time instant \n",
    "# for which you want to plot the spectrum\n",
    "# t = 0.45 \n",
    "print('Plotting spectrum at approximately', \"{:.2f}\".format(t), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4ff09-8c0f-4214-a989-9ab1eea7918c",
   "metadata": {},
   "source": [
    "Convert this to a frame index.  <tt>len(assassine)</tt> is the number of time samples. <tt>dsr</tt> is the sampling rate. <tt>S_assassine.shape[1]</tt> is the number of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee4a04-7cb8-4c13-bf82-8af3580b9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx=int(t/(len(assassine)/dsr)*S_assassine.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f4572-4fa9-4c0c-963f-36f66a4fa4af",
   "metadata": {},
   "source": [
    "Now plot the spectrum for this frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63934f7a-82ed-4ca7-906d-d39de1b32aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.plot(S_assassine_db[:,frame_idx:frame_idx+1])\n",
    "ax.set(title='Spectrum')\n",
    "x_tick_pos=[n for n in range(0, dn_fft // 2 + 1, dn_fft // 16)]\n",
    "x_tick_lbl=[str(dsr / dn_fft * n) + 'Hz' for n in x_tick_pos]\n",
    "ax.set_xticks(x_tick_pos)\n",
    "ax.set_xticklabels(x_tick_lbl)\n",
    "ax.set_xlim([0, dn_fft/2])\n",
    "ax.set(xlabel='Frequency')\n",
    "ax.set(ylabel='Amplitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f1aea-436b-47eb-8523-ef905810d2cd",
   "metadata": {},
   "source": [
    "Answer the following questions by plotting the spectrum for different frames:\n",
    "- What characteristics differentiate voiced and un-voiced sounds in the frequency domain?\n",
    "- What is the approximate funamental frequency *F*<sub>0</sub> for the first ‘a’ of ‘assassiner’?\n",
    "- What are the approximate frequencies of the three first formants of the second ‘a’ of ‘assassiner’?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c44fd0-9d9b-4356-81c6-87c08efaf782",
   "metadata": {},
   "source": [
    "# Linear prediction and linear predictive coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a92f001-1cab-4459-9ccc-7075e97766ee",
   "metadata": {},
   "source": [
    "We will see an example of linear predictive encoding and decoding, also referred to as analysis and synthesis, with different prediction orders. We are interested in seeing what is the *error*, or the *excitation* and will compare this for different prediction orders in terms of the mean square error.  \n",
    "\n",
    "We will start by estimating the filter parameters <tt>a</tt>, the gain <tt>g</tt> and the error <tt>e</tt> for a 10th order filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b718d-c8bb-492e-962c-385b26fc3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change the prediction order to observe the effect\n",
    "prediction_order = 10\n",
    "y=assassine\n",
    "[a, g, e] = lpc.lpcfit(y, prediction_order)\n",
    "# and now resynthesize an estimate y_hat of the \n",
    "# original signal using the same a, g, and e\n",
    "y_hat = lpc.lpcsynth(a, g, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc3032e-5ac2-486d-b612-0362eabef31c",
   "metadata": {},
   "source": [
    "Listen to the result.  If all worked well, it should be of the same or similar quality as the original signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3bf406-2ef8-4eb6-9f71-1d2390bc1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=y_hat, rate=dsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ab62c-0d5d-4c27-b4f3-273abf8f7010",
   "metadata": {},
   "source": [
    "Now we'll plot the original signal, the LPC approximation and the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca55f2b-1a68-4351-9ba6-6f1f7c509834",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(15, 6))\n",
    "# due to framing, the lengths are unlikely to the same\n",
    "# find the length of the shortest file\n",
    "l=min(y.size,y_hat.size)\n",
    "librosa.display.waveshow(y[:l], sr=dsr, ax=ax[0], label='Original')\n",
    "librosa.display.waveshow(y_hat[:l], sr=dsr, ax=ax[0], label='LPC')\n",
    "librosa.display.waveshow(y[:l]-y_hat[:l], sr=dsr, ax=ax[1], label='Error')\n",
    "ax[0].set_xlim([0, y.size/dsr]);\n",
    "ax[0].set(xlabel=None);\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "# compute the mean square error\n",
    "print('Mean error is', \"{:.5f}\".format(np.mean(np.square(y[:1]-y_hat[:l]))))\n",
    "# we don't use e directly since this is normalised individually for each frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b7c3f-07a4-4d57-9b6f-4faf5abf8871",
   "metadata": {},
   "source": [
    "Now we're going to look at an invidual frame in the same way we did earlier in comparing voiced and unvoiced speech.  Once again, we'll initially choose a frame at random, but you should change the parameter to select a specific interval you want to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4063f774-bb8b-4041-9a07-d2776fa14479",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = random.uniform(0, 1)*len(assassine)/dsr \n",
    "# uncomment and change the following to whatever time instant \n",
    "# for which you want to plot the spectrum\n",
    "#t = 0.07\n",
    "print('Plotting spectrum at approximately', \"{:.2f}\".format(t), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c4ba2-adea-4066-aed4-602333224470",
   "metadata": {},
   "source": [
    "Now we'll extract the frame at this time instant and then plot and compare DFT-derived and LPC-derived spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c55b0d-9451-4e63-887b-49016cc55beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=librosa.util.frame(y, frame_length=dn_fft, hop_length=dhop_length)\n",
    "frame_idx=int(t/(len(y)/dsr)*frames.shape[1])\n",
    "frame_data=frames[:,frame_idx]\n",
    "\n",
    "# compute the dft-derived magnitude spectrum\n",
    "dft_spectrum=np.abs(scipy.fft.fft(frame_data, n=dn_fft))\n",
    "\n",
    "# compute the lpc-derived magnitude spectrum\n",
    "prediction_order=50   # don't set too high (<50), otherwise the order\n",
    "                      # will be greater than the number of samples in \n",
    "                      # a single pitch interval (at 8 kHz samp. rate)\n",
    "a = librosa.lpc(frame_data, order=prediction_order)  \n",
    "h = np.abs(1/scipy.fft.fft(a, n=dn_fft))\n",
    "\n",
    "# compute the lpc approximation\n",
    "b = np.hstack([[0], -1 * a[1:]])\n",
    "frame_data_hat = scipy.signal.lfilter(b, [1], frame_data)\n",
    "print('Mean error is', \"{:.5f}\".format(\n",
    "    np.mean(np.square(frame_data-frame_data_hat))))\n",
    "\n",
    "# plot the frame waveform and lpc-derived approximation\n",
    "fig, ax = plt.subplots(nrows=3, figsize=(15, 12))\n",
    "librosa.display.waveshow(frame_data, sr=dsr, ax=ax[0], \n",
    "                         x_axis=None, label='Frame waveform')\n",
    "librosa.display.waveshow(frame_data_hat, sr=dsr, ax=ax[0], \n",
    "                         x_axis=None, label='LPC estimate')\n",
    "librosa.display.waveshow(frame_data-frame_data_hat, sr=dsr, \n",
    "                         ax=ax[1], label='Error')\n",
    "ax[0].legend(); ax[1].legend()\n",
    "ax[0].set(xlabel=None);\n",
    "ax[0].set_xlim([0, frame_data.size/dsr])\n",
    "ax[1].set_xlim([0, frame_data.size/dsr])\n",
    "\n",
    "# plot the two spectra, resampling the dft_spectrum to the same size\n",
    "ax[2].plot(abs(h),label='LPC-derived spectrum')\n",
    "ax[2].plot(dft_spectrum, label='DFT-derived spectrum')\n",
    "x_tick_pos=[n for n in range(0, dn_fft //2 +1, dn_fft // 16)]\n",
    "x_tick_lbl=[str(dsr / dn_fft * n) + 'Hz' for n in x_tick_pos]\n",
    "ax[2].set_xticks(x_tick_pos)\n",
    "ax[2].set_xticklabels(x_tick_lbl)\n",
    "ax[2].set_xlim([0, dn_fft / 2])\n",
    "ax[2].legend();\n",
    "ax[2].set(xlabel='Frequency')\n",
    "ax[2].set(ylabel='Amplitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863e9fb-8bba-4a6e-9683-fd9c3013d085",
   "metadata": {},
   "source": [
    "Plot the DFT-derived and LPC-derived spectra for different frames containing either voiced or unvoiced speech and for different prediction orders and then describe and account for the differences you observe.  What is the relationship between the number of detected formants and the order of the prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b5442-6123-4442-929a-d4f1714ae78f",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692df1bb-5625-45fe-80d0-d30b44b0cbfe",
   "metadata": {},
   "source": [
    "We'll examine the LPC approximation errors for four different signals ‹a›, ‹ch›, ‹whistle› and ‹a – saturated› and for predictions of order 12.  We'll load the four waveforms, cut each to a consistent, short duration of 0.1 seconds and normalise them to have peak, unity amplitude.  Then we'll derive the prediction coefficients and resynthesize LPC approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff390651-c127-49b4-9b22-bf858f5d0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_order=12\n",
    "n_fft=1024\n",
    "duration=0.1\n",
    "\n",
    "# for the vowel a\n",
    "y_a, dsr = librosa.load('a.wav', sr=dsr, duration=duration)\n",
    "y_a = y_a / max(y_a)\n",
    "a = librosa.lpc(y_a, order=prediction_order) \n",
    "b = np.hstack([[0], -1 * a[1:]])\n",
    "y_a_hat = scipy.signal.lfilter(b, [1], y_a)\n",
    "\n",
    "# for the consonant ch\n",
    "y_ch, dsr = librosa.load('ch.wav', sr=dsr, duration=duration)\n",
    "y_ch = y_ch / max(y_ch)\n",
    "a = librosa.lpc(y_ch, order=prediction_order) \n",
    "b = np.hstack([[0], -1 * a[1:]])\n",
    "y_ch_hat = scipy.signal.lfilter(b, [1], y_ch)\n",
    "\n",
    "# for the whistle\n",
    "y_whistle, dsr = librosa.load('siffle.wav', sr=dsr, duration=duration)\n",
    "y_whistle = y_whistle / max(y_whistle)\n",
    "a = librosa.lpc(y_whistle, order=prediction_order) \n",
    "b = np.hstack([[0], -1 * a[1:]])\n",
    "y_whistle_hat = scipy.signal.lfilter(b, [1], y_whistle)\n",
    "\n",
    "# for the saturated vowel a\n",
    "y_asat, dsr = librosa.load('asat.wav', sr=dsr, duration=duration)\n",
    "y_asat = y_asat / max(y_asat)\n",
    "a = librosa.lpc(y_asat, order=prediction_order) \n",
    "b = np.hstack([[0], -1 * a[1:]])\n",
    "y_asat_hat = scipy.signal.lfilter(b, [1], y_asat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969b05b-0bfb-492c-a101-0ba632a04cd3",
   "metadata": {},
   "source": [
    "Now we'll plot the normalised waveforms (blue profiles) and superimpose upon them the error (orange profiles), namely the difference between the original waveforms and the LPC approximations.  We'll also compute the mean square error in each case (shown in the respective legends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1c155-bb19-4a6e-b471-d140249948b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, sharex=True, sharey=True, figsize=(15, 10))\n",
    "\n",
    "librosa.display.waveshow(y_a, sr=dsr, ax=ax[0], label='a')\n",
    "z=np.mean(np.square(y_a-y_a_hat))\n",
    "librosa.display.waveshow(y_a-y_a_hat, sr=dsr, ax=ax[0], \n",
    "                         label='error '+\"{:.3f}\".format((z)))\n",
    "\n",
    "librosa.display.waveshow(y_ch, sr=dsr, ax=ax[1], label='ch')\n",
    "z=np.mean(np.square(y_ch-y_ch_hat))\n",
    "librosa.display.waveshow(y_ch-y_ch_hat, sr=dsr, ax=ax[1], \n",
    "                         label='error '+\"{:.3f}\".format((z)))\n",
    "\n",
    "librosa.display.waveshow(y_whistle, sr=dsr, ax=ax[2], \n",
    "                         label='whistle')\n",
    "z=np.mean(np.square(y_whistle-y_whistle_hat))\n",
    "librosa.display.waveshow(y_whistle-y_whistle_hat, sr=dsr, \n",
    "                         ax=ax[2], label='error '+\"{:.3f}\".format((z)))\n",
    "\n",
    "librosa.display.waveshow(y_asat, sr=dsr, ax=ax[3], \n",
    "                         label='a saturated')\n",
    "z=np.mean(np.square(y_asat-y_asat_hat))\n",
    "librosa.display.waveshow(y_asat-y_asat_hat, sr=dsr, ax=ax[3], \n",
    "                         label='error '+\"{:.3f}\".format((z)))\n",
    "\n",
    "ax[0].legend(); ax[1].legend(); ax[2].legend(); ax[3].legend()\n",
    "ax[0].set(xlabel=None); ax[1].set(xlabel=None); ax[2].set(xlabel=None);\n",
    "ax[0].set_xlim([0, duration]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfd8b9-b89c-4575-925c-78a3eca93c10",
   "metadata": {},
   "source": [
    "Comment on and compare the results for each of the four waveforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f911308-1b8e-4c26-9327-f57be233d9aa",
   "metadata": {},
   "source": [
    "## Cross-synthesis\n",
    "Now we'll generate the cross-synthesis examples that we looked at in your lectures.  The objective is to reinforce your understanding of what are the two components in the source-filter model.  We'll start by loading the same speech utterance used in the lecture examples, namely the utterance '*the football team coach has a watch thin as a dime*' spoken by an American male speaker, a short snippet of piano music and a pulse train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df4455e-6d15-41df-bfcb-221c65c41155",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_speech, sr_speech = librosa.load('mpgr1_sx419-8k.wav', sr=dsr)\n",
    "y_bach, sr_bach = librosa.load('bach.8k.wav', sr=dsr)\n",
    "y_pulseTrain, sr_pulseTrain = librosa.load('pulseTrain.wav', sr=dsr)\n",
    "y_noise = np.random.normal(0,1,4*dsr)\n",
    "\n",
    "# cut all waveforms to the same minimum length\n",
    "min_l = min(y_speech.size, y_bach.size, y_pulseTrain.size, y_noise.size)\n",
    "y_speech = y_speech[:min_l]\n",
    "y_bach = y_bach[:min_l]\n",
    "y_pulseTrain = y_pulseTrain[:min_l]\n",
    "y_noise = y_noise[:min_l]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01dd51a-d1c9-4ff6-80d5-f4a45695fac1",
   "metadata": {},
   "source": [
    "Now we'll perform LPC analysis of all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e734f-a41b-407f-b77b-0cd56b540ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[a_speech, g_speech, e_speech] = lpc.lpcfit(y_speech, 20)\n",
    "[a_bach, g_bach, e_bach] = lpc.lpcfit(y_bach, 20)\n",
    "[a_pulseTrain, g_pulseTrain, e_pulseTrain] = lpc.lpcfit(y_pulseTrain, 20)\n",
    "[a_noise, g_noise, e_noise] = lpc.lpcfit(y_noise, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac552997-c162-410e-80e1-c03b9baab2a2",
   "metadata": {},
   "source": [
    "Now we resynthesise an estimate of the original speech as a reference and listen to it.  It should sound like high quality, natural speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74cb2d-900a-42b5-bed5-bada6d6beecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_speech_hat = lpc.lpcsynth(a_speech, g_speech, e_speech)\n",
    "IPython.display.Audio(data=y_speech_hat, rate=dsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a830f2-8453-43ba-ba2c-e8a914605791",
   "metadata": {},
   "source": [
    "Now we'll plot the waveforms and spectrograms for the original and resynthesised utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e1e69-48d4-4700-87ae-1dab0d7a6110",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 5))\n",
    "\n",
    "# the original\n",
    "librosa.display.waveshow(y_speech, sr=sr_bach, ax=ax[0,0], \n",
    "                         x_axis=None, label='Original')\n",
    "ax[0,0].legend()\n",
    "ax[0,0].set(xlabel=None);\n",
    "S_speech = np.abs(librosa.stft(y_speech, n_fft=dn_fft, \n",
    "                               hop_length=dhop_length))\n",
    "S_speech_db = librosa.amplitude_to_db(S_speech, ref=np.max)\n",
    "librosa.display.specshow(S_speech_db, n_fft=dn_fft, \n",
    "                         hop_length=dhop_length, sr=sr_bach, x_axis=None, \n",
    "                         y_axis='linear', ax=ax[0,1], cmap='jet')\n",
    "ax[0,1].set(xlabel=None);\n",
    "\n",
    "# the approximation\n",
    "librosa.display.waveshow(y_speech_hat, sr=sr_bach, ax=ax[1,0], \n",
    "                         label='LPC approximation')\n",
    "ax[1,0].legend()\n",
    "S_speech_hat = np.abs(librosa.stft(y_speech_hat, n_fft=dn_fft, \n",
    "                                   hop_length=dhop_length))\n",
    "S_speech_hat_db = librosa.amplitude_to_db(S_speech_hat, ref=np.max)\n",
    "librosa.display.specshow(S_speech_hat_db, n_fft=dn_fft, \n",
    "                         hop_length=dhop_length, sr=sr_bach, \n",
    "                         x_axis='time', y_axis='linear', \n",
    "                         ax=ax[1,1], cmap='jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ac755-35fb-4946-9f69-901ae26b6b5c",
   "metadata": {},
   "source": [
    "As expected, we see very little difference between the two.\n",
    "\n",
    "Now for the cross synthesis. We will resynthesise new waveforms by mixing the LPC coefficients generated from the speech signal with excitation components generated from other signals and listen to the results. Think about the difference between the source and the filter and the influence of both upon what you will hear.  We'll start by using the *excitation* from a short snippet of Bach piano music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f8c04-6180-4202-957c-1bab43f667dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=e_bach, rate=dsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72819289-041f-4380-aa85-0e096b076951",
   "metadata": {},
   "source": [
    "This won't sound great, at least certainly not as high quality as the original signal.  Now we'll perform the cross-synthesis using the filter information of the speech signal and the excitation of the piano music.  We'll plot waveforms and spectrograms of the piano music excitation and then the cross-synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9d409-9b69-4f24-a674-095b9a3e54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_speech_hat_e_bach = lpc.lpcsynth(a_speech, g_speech, e_bach)\n",
    "\n",
    "# plot the time waveforms\n",
    "fig, ax = plt.subplots(nrows=2,ncols=2,figsize=(15, 5))\n",
    "librosa.display.waveshow(e_bach, sr=sr_bach, ax=ax[0,0], \n",
    "                         x_axis=None, label='Bach excitation')\n",
    "librosa.display.waveshow(y_speech_hat_e_bach, sr=sr_bach, \n",
    "                         ax=ax[0,1], x_axis=None, \n",
    "                         label='Bach excited speech')\n",
    "ax[0,0].legend()\n",
    "ax[0,1].legend()\n",
    "\n",
    "# and the corresponding spectrogams\n",
    "S_e_bach = np.abs(librosa.stft(e_bach, n_fft=dn_fft, \n",
    "                               hop_length=dhop_length))\n",
    "S_e_bach_db = librosa.amplitude_to_db(S_e_bach, ref=np.max)\n",
    "librosa.display.specshow(S_e_bach_db, n_fft=dn_fft, \n",
    "                         hop_length=dhop_length, sr=sr_speech, \n",
    "                         x_axis='time', y_axis='linear', \n",
    "                         ax=ax[1,0], cmap='jet')\n",
    "S_y_speech_hat_e_bach = np.abs(librosa.stft(\n",
    "    y_speech_hat_e_bach, n_fft=dn_fft, hop_length=dhop_length))\n",
    "S_y_speech_hat_e_bach_db = librosa.amplitude_to_db(\n",
    "    S_y_speech_hat_e_bach, ref=np.max)\n",
    "librosa.display.specshow(\n",
    "    S_y_speech_hat_e_bach_db, n_fft=dn_fft, \n",
    "    hop_length=dhop_length, sr=sr_speech, x_axis='time', \n",
    "    y_axis='linear', ax=ax[1,1], cmap='jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c2264-fa55-4574-bbc2-218dc337e4dc",
   "metadata": {},
   "source": [
    "Ignore the scale of the excitation plot; it has been normalised at the frame-level.  Now we'll listen to the resynthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0484a4b-c325-4405-b37e-41a384adf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=y_speech_hat_e_bach, rate=dsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58351874-2a22-431b-b43f-0950b1fd41aa",
   "metadata": {},
   "source": [
    "Hopefully, you'll now have a more intuitive understanding of what is the source and what is the filter.\n",
    "\n",
    "To reinforce this understanding, we'll do the same using the excitation signal of the pulse train. Here's the pulse train itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6035bb10-f021-45ac-93b8-00ceddf91ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=y_pulseTrain, rate=dsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc1202-8877-4fcb-acf1-c765e34b647d",
   "metadata": {},
   "source": [
    "Any now for the resynthesis.  Again, we'll plot waveforms and spectrograms of the pulse train excitation and then the cross-synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e20d7-7a20-4fe1-9fee-58e77cfec496",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_speech_hat_e_pulseTrain = lpc.lpcsynth(a_speech, g_speech, e_pulseTrain)\n",
    "\n",
    "# plot the time waveforms\n",
    "fig, ax = plt.subplots(nrows=2,ncols=2,figsize=(15, 5))\n",
    "librosa.display.waveshow(e_pulseTrain, sr=sr_bach, ax=ax[0,0], \n",
    "                         x_axis=None, label='Pulse train excitation')\n",
    "librosa.display.waveshow(y_speech_hat_e_pulseTrain, sr=sr_bach,\n",
    "                         ax=ax[0,1], x_axis=None,\n",
    "                         label='Pulse train excited speech')\n",
    "ax[0,0].legend()\n",
    "ax[0,1].legend()\n",
    "\n",
    "# and the corresponding spectrogams\n",
    "S_e_pulseTrain = np.abs(librosa.stft(e_pulseTrain, n_fft=dn_fft,\n",
    "                                     hop_length=dhop_length))\n",
    "S_e_pulseTrain_db = librosa.amplitude_to_db(\n",
    "    S_e_pulseTrain, ref=np.max)\n",
    "librosa.display.specshow(S_e_pulseTrain_db, n_fft=dn_fft, \n",
    "                         hop_length=dhop_length, sr=sr_speech, \n",
    "                         x_axis='time', y_axis='linear', \n",
    "                         ax=ax[1,0], cmap='jet')\n",
    "S_y_speech_hat_e_pulseTrain = np.abs(librosa.stft(\n",
    "    y_speech_hat_e_pulseTrain, n_fft=dn_fft, hop_length=dhop_length))\n",
    "S_y_speech_hat_e_pulseTrain_db = librosa.amplitude_to_db(\n",
    "    S_y_speech_hat_e_pulseTrain, ref=np.max)\n",
    "librosa.display.specshow(S_y_speech_hat_e_pulseTrain_db, n_fft=dn_fft, \n",
    "                         hop_length=dhop_length, sr=sr_speech, \n",
    "                         x_axis='time', y_axis='linear', ax=ax[1,1], \n",
    "                         cmap='jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606e0d4-9ba5-4172-a42f-58d9e587910b",
   "metadata": {},
   "source": [
    "Try to predict how will sound the resynthesis before you listen to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013eb98-f7b6-43f2-aaaf-31d6496598c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=y_speech_hat_e_pulseTrain, rate=dsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e444b-f27c-4cbb-a83a-8a590b498990",
   "metadata": {},
   "source": [
    "Finally, here's the same style of cross-synthesis using excitation from a white noise signal.  Here it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c089e4-a271-4608-bc89-d17a5afca3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=e_noise, rate=dsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912619d1-77c3-4b81-8052-d1df6bba07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_speech_hat_e_noise = lpc.lpcsynth(a_speech, g_speech, e_noise)\n",
    "\n",
    "# plot the time waveforms\n",
    "fig, ax = plt.subplots(nrows=2,ncols=2,figsize=(15, 5))\n",
    "librosa.display.waveshow(e_noise, sr=sr_speech, ax=ax[0,0], \n",
    "                         x_axis=None, label='Noise excitation')\n",
    "librosa.display.waveshow(y_speech_hat_e_noise, sr=sr_speech, \n",
    "                         ax=ax[0,1], x_axis=None, \n",
    "                         label='Noise excited speech')\n",
    "ax[0,0].legend()\n",
    "ax[0,1].legend()\n",
    "\n",
    "# and the corresponding spectrogams\n",
    "S_e_noise = np.abs(librosa.stft(e_noise, n_fft=dn_fft, \n",
    "                                hop_length=dhop_length))\n",
    "S_e_noise_db = librosa.amplitude_to_db(S_e_noise, ref=np.max)\n",
    "librosa.display.specshow(S_e_noise_db, n_fft=dn_fft, \n",
    "                         hop_length=dhop_length, sr=sr_speech, \n",
    "                         x_axis='time', y_axis='linear', \n",
    "                         ax=ax[1,0], cmap='jet')\n",
    "S_y_speech_hat_e_noise = np.abs(librosa.stft(\n",
    "    y_speech_hat_e_noise, n_fft=dn_fft, hop_length=dhop_length))\n",
    "S_y_speech_hat_e_noise_db = librosa.amplitude_to_db(\n",
    "    S_y_speech_hat_e_noise, ref=np.max)\n",
    "librosa.display.specshow(S_y_speech_hat_e_noise_db, \n",
    "                         n_fft=dn_fft, hop_length=dhop_length, \n",
    "                         sr=sr_speech, x_axis='time', \n",
    "                         y_axis='linear', ax=ax[1,1], \n",
    "                         cmap='jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18acee34-16c0-4b58-bde2-6570a121b643",
   "metadata": {},
   "source": [
    "Here's the cross-synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7eea05-9062-4631-abe1-5ab77a307104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=y_speech_hat_e_noise, rate=dsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79397b7-30f5-402a-8b37-fbe603a06f08",
   "metadata": {},
   "source": [
    "Briefly describe what you heard in the examples above and then describe in your own words what are the source and the filter in the source-filter model and their physiological origins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d605cc7-d6e9-426a-bf2b-682b32f8f03d",
   "metadata": {},
   "source": [
    "## LPC spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c664e-40c2-480a-86a9-db2bab67c0e0",
   "metadata": {},
   "source": [
    "Since we can estimate spectra using LPC, we can produce spectrograms too.  We'll now do this for the short *assassiner* utterance from earlier. We'll start by deriving the prediction coefficients and will then plot and compare LPC-derived and DFT-derived spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdcba09-ca9f-4f76-a2fc-c3a062303635",
   "metadata": {},
   "outputs": [],
   "source": [
    "[a, g, e] = lpc.lpcfit(assassine, 20, h=dhop_length, w=dn_fft)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, sharey=True,figsize=(15, 3))\n",
    "\n",
    "# compute and plot the LPC spectrogram\n",
    "S=np.abs(scipy.fft.fft(a[0,:], n=dn_fft))\n",
    "for i in range(1,a.shape[0]):\n",
    "    S = np.vstack((S,np.abs(1/scipy.fft.fft(a[i,:], n=dn_fft))))\n",
    "S_db = librosa.amplitude_to_db(S[:,0:(dn_fft//2+1)].T, ref=np.max)\n",
    "librosa.display.specshow(S_db, n_fft=dn_fft, hop_length=dhop_length, \n",
    "                         sr=sr_speech, x_axis='time', \n",
    "                         y_axis='linear', ax=ax[0], cmap='jet')\n",
    "ax[0].set(title='LPC spectrogram')\n",
    "\n",
    "# compute and plot the DFT spectrogram\n",
    "S_assassine = np.abs(librosa.stft(y, center=True, n_fft=dn_fft, \n",
    "                                  hop_length=dhop_length))\n",
    "S_assassine_db = librosa.amplitude_to_db(\n",
    "    S_assassine, ref=np.max)                                                                  \n",
    "librosa.display.specshow(\n",
    "    S_assassine_db, n_fft=dn_fft, hop_length=dhop_length, \n",
    "    sr=dsr, x_axis='time', y_axis='linear', ax=ax[1], cmap='jet')\n",
    "ax[1].set(title='DFT spectrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc67f6-247b-4086-8239-0ef6418e8bf4",
   "metadata": {},
   "source": [
    "Explain the principal differences you observe between the two spectrograms and describe their origins as regards both the human speech production process and the tools used for signal analyis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ec22d-ccb8-45d1-97a3-1193d23423bb",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "We will now look at how representations of speech, such as those derived using LPC analysis, can be of use in modelling and classifying/recognising speech signals.  We will look at estimating the formant positions of three different vowel sounds, specifically the three vowels we looked at in lectures, the /aa/, /er/ and /iy/ vowels.  We'll plot them and listen to them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54c4e2-e283-4880-a132-9d151bc17d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vowel sounds sourced from\n",
    "# https://en.wikipedia.org/wiki/Help:IPA\n",
    "# and converted to wav format\n",
    "# used under the following licenses\n",
    "# https://en.wikipedia.org/wiki/File:Open_back_unrounded_vowel.ogg\n",
    "# https://en.wikipedia.org/wiki/File:Open-mid_central_unrounded_vowel.ogg\n",
    "# https://en.wikipedia.org/wiki/File:Close_front_unrounded_vowel.ogg\n",
    "y_aa, dsr = librosa.load('Open_back_unrounded_vowel.wav', sr=dsr)\n",
    "y_er, dsr = librosa.load('Open-mid_central_unrounded_vowel.wav', sr=dsr)\n",
    "y_iy, dsr = librosa.load('Close_front_unrounded_vowel.wav', sr=dsr)\n",
    "\n",
    "# make them the same, shortest length\n",
    "l=min(y_aa.size,y_er.size,y_iy.size)\n",
    "y_aa=y_aa[:l]\n",
    "y_er=y_er[:l]\n",
    "y_iy=y_iy[:l]\n",
    "\n",
    "# plot the files\n",
    "fig, ax = plt.subplots(ncols=3, sharey=True, figsize=(20, 3))\n",
    "librosa.display.waveshow(y_aa, sr=dsr, ax=ax[0], label='/aa/')\n",
    "librosa.display.waveshow(y_er, sr=dsr, ax=ax[1], label='/er/')\n",
    "librosa.display.waveshow(y_iy, sr=dsr, ax=ax[2], label='/iy/')\n",
    "ax[0].legend(); ax[1].legend(); ax[2].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c562511-0e90-45e8-a4c9-eedbbf388021",
   "metadata": {},
   "source": [
    "Listen to the /aa/ sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3f28e-1933-4ab5-bbbd-77bd8c2cc8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=y_aa, rate=dsr) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac7b10-02d7-4e5e-978e-4c4adf29d981",
   "metadata": {},
   "source": [
    "Listen to the /er/ sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c0275-6daa-4fb2-a4cc-9573383e6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=y_er, rate=dsr) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c07dd-7433-4359-b749-4ebc6785ac09",
   "metadata": {},
   "source": [
    "Listen to the /iy/ sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3343a-6737-4e05-964f-84aa1eb5e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=y_iy, rate=dsr) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37d0a7-5545-416a-ba46-6590829f3255",
   "metadata": {},
   "source": [
    "Now we'll extract frame-level LPC coefficients from each of the three audio files and *estimate* the frequencies of the first two formants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827c9d2-f524-44c9-bc07-636c56b4f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default enframing and lpc parameters\n",
    "hop_length=16\n",
    "lpc_order=8\n",
    "n_formants=int((lpc_order+2)/2)\n",
    "\n",
    "# estimate the filter parameters a for each sound\n",
    "[a_aa, g_aa, e_aa] = lpc.lpcfit(y_aa, lpc_order, h=hop_length, w=dn_fft)\n",
    "[a_er, g_er, e_er] = lpc.lpcfit(y_er, lpc_order, h=hop_length, w=dn_fft)\n",
    "[a_iy, g_iy, e_iy] = lpc.lpcfit(y_iy, lpc_order, h=hop_length, w=dn_fft)\n",
    "\n",
    "# and now estimate the formant frequencies\n",
    "# for /aa/\n",
    "frqs_aa = np.empty((0,n_formants), float)\n",
    "for i in range(a_aa.shape[0]):\n",
    "    # compute the complex roots of the polynomial\n",
    "    r_aa=np.roots(a_aa[i,:])\n",
    "    # select those in the top half of the z-pland\n",
    "    rts_aa = [r for r in r_aa if np.imag(r) >= 0]\n",
    "    # ensure the correct number of array elements\n",
    "    rts_aa = np.pad(rts_aa, (0,n_formants-len(rts_aa)))\n",
    "    # determine the angles\n",
    "    angz_aa = np.arctan2(np.imag(rts_aa), np.real(rts_aa))\n",
    "    # the frequencies in Hz\n",
    "    frq = np.array([sorted(angz_aa * (sr / (2 * np.pi)))])\n",
    "    # remove any below 100 Hz\n",
    "    frq = np.roll(frq, -sum(frq < 100))\n",
    "    # append to the matrix\n",
    "    frqs_aa=np.append(frqs_aa, frq, axis=0)\n",
    "# for /er/\n",
    "frqs_er = np.empty((0,n_formants), float)\n",
    "for i in range(a_er.shape[0]):\n",
    "    # compute the complex roots of the polynomial\n",
    "    r_er=np.roots(a_er[i,:])\n",
    "    # select those in the top half of the z-pland\n",
    "    rts_er = [r for r in r_er if np.imag(r) >= 0]\n",
    "    # ensure the correct number of array elements\n",
    "    rts_er = np.pad(rts_er, (0,n_formants-len(rts_er)))\n",
    "    # determine the angles\n",
    "    angz_er = np.arctan2(np.imag(rts_er), np.real(rts_er))\n",
    "    # the frequencies in Hz\n",
    "    frq = np.array([sorted(angz_er * (sr / (2 * np.pi)))])\n",
    "    # remove any below 100 Hz\n",
    "    frq = np.roll(frq, -sum(frq < 100))\n",
    "    # append to the matrix\n",
    "    frqs_er=np.append(frqs_er, frq, axis=0)\n",
    "# for /iy/\n",
    "frqs_iy = np.empty((0,n_formants), float)\n",
    "for i in range(a_iy.shape[0]):\n",
    "    # compute the complex roots of the polynomial\n",
    "    r_iy=np.roots(a_iy[i,:])\n",
    "    # select those in the top half of the z-pland\n",
    "    rts_iy = [r for r in r_iy if np.imag(r) >= 0]\n",
    "    # ensure the correct number of array elements\n",
    "    rts_iy = np.pad(rts_iy, (0,n_formants-len(rts_iy)))\n",
    "    # determine the angles\n",
    "    angz_iy = np.arctan2(np.imag(rts_iy), np.real(rts_iy))\n",
    "    # the frequencies in Hz\n",
    "    frq = np.array([sorted(angz_iy * (sr / (2 * np.pi)))])\n",
    "    # remove any below 100 Hz\n",
    "    frq = np.roll(frq, -sum(frq < 100))\n",
    "    # append to the matrix\n",
    "    frqs_iy=np.append(frqs_iy, frq, axis=0)\n",
    "# yes, we could have done this more efficiently\n",
    "# with a function, but this isn't an exercise in python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ed818-a47c-4902-93fa-9e7e2e667084",
   "metadata": {},
   "source": [
    "We'll now plot the frame-level formant estimates for each of the three sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfb743-17f0-43bc-934d-4cefc32a25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set(xlabel='F1 (Hz)')\n",
    "ax.set(ylabel='F2 (Hz)')\n",
    "\n",
    "# Let's remove coefficients toward the beginning and end of each sound.\n",
    "# These coefficients correspond more to onsets/offsets or transitions\n",
    "# rather than more 'pure' vowel sounds\n",
    "frqs_aa=frqs_aa[int(frqs_aa.shape[0]*0.2):int(frqs_aa.shape[0]*0.8),:]\n",
    "frqs_er=frqs_er[int(frqs_er.shape[0]*0.2):int(frqs_er.shape[0]*0.8),:]\n",
    "frqs_iy=frqs_iy[int(frqs_iy.shape[0]*0.2):int(frqs_iy.shape[0]*0.8),:]\n",
    "\n",
    "# now a scatter plot for each 'cluster' of formant positions\n",
    "ax.scatter(frqs_aa[:,0],frqs_aa[:,1], label='/aa/')\n",
    "ax.scatter(frqs_er[:,0],frqs_er[:,1], label='/er/')\n",
    "ax.scatter(frqs_iy[:,0],frqs_iy[:,1], label='/iy/')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cff50-7630-4d09-a3a7-62252b29253f",
   "metadata": {},
   "source": [
    "Comment on the results and the potential for characterisations of the vocal tract or filter configuration can be used for speech recognition.\n",
    "\n",
    "Next, we're going to see how we might design a trivial classifier to recognise these different vowel sounds using the kmeans clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c54339-f364-4989-955d-62a54594bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's stack the formant estimates for each sound into \n",
    "# a single matrix\n",
    "X = np.vstack((frqs_aa[:,0:2], frqs_er[:,0:2], frqs_iy[:,0:2]))\n",
    "\n",
    "# perform a kmeans clustering on the data\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "labels = kmeans.fit(X).predict(X)\n",
    "\n",
    "# and plot the resulting clusters\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set(xlabel='F1 (Hz)')\n",
    "ax.set(ylabel='F2 (Hz)')\n",
    "ax.scatter(X[:, 0], X[:, 1], c=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52778dee-0b12-424f-962d-2d5e881b66b1",
   "metadata": {},
   "source": [
    "Describe how you would implement an automatic speech recognition system using such features.  How might it perform?\n",
    "\n",
    "Repeat this exercise after adjusting the code to plot the same features for the full recordings (not the trimmed versions) and comment on the results.  How would an automatic speech recognition system perform now?  Do formant estimates make for suitable and sufficient features for speech recognition?  Can you think of how you might do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e975c-c482-4dd2-9eec-ba5513dc35c7",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "We will look at a few different approaches to feature extraction that we might use for modelling and classification/recognition.  One involves the use of LPC coefficients.  We'll also look at Mel-scaling, at the liftering of DFT-derived spectra and cepstral processing.  Last, we'll look at feature correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cb07a-d236-4d27-a7f4-db8058792246",
   "metadata": {},
   "source": [
    "## Mel scaling\n",
    "Using the short <tt>assassiner</tt> utterance from earlier, we will plot and compare the DFT-derived spectrogram of a speech signal to a Mel-scaled spectrogram.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f33f57-5039-41e1-a5b0-e436470aeb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, sharey=True, figsize=(15, 3))\n",
    "\n",
    "# plot the DFT spectrogram\n",
    "S = np.abs(librosa.stft(assassine, n_fft=dn_fft, \n",
    "                        hop_length=dhop_length))\n",
    "S_db = librosa.amplitude_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_db, n_fft=dn_fft, hop_length=dhop_length, \n",
    "                         sr=dsr, x_axis='time', y_axis='linear', \n",
    "                         ax=ax[0], cmap='jet')\n",
    "ax[0].set(title='DFT spectrogram')\n",
    "\n",
    "# plot the mel-scaled DFT spectrogram\n",
    "S = librosa.feature.melspectrogram(\n",
    "    y=assassine, sr=dsr, n_fft=dn_fft, hop_length=dhop_length, \n",
    "    n_mels=32, fmax=dsr/2)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "img=librosa.display.specshow(\n",
    "    S_dB, n_fft=dn_fft, hop_length=dhop_length, x_axis='time', \n",
    "    y_axis='mel', sr=dsr, ax=ax[1], cmap='jet', fmax=dsr/2)\n",
    "ax[1].set(title='Mel-frequency spectrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4d477-4c66-4963-8a46-56bdf09c35b2",
   "metadata": {},
   "source": [
    "Plot Mel-spectrograms with different numbers of Mel filters and comment on the results.\n",
    "\n",
    "Now let's look at the Mel-spectrogram of the earler linear chirp signal.  Again, we'll plot both the DFT spectrogram and the Mel-spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900f0fd-d597-42ca-a8a3-2ba011fab824",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, sharey=True, figsize=(15, 3))\n",
    "\n",
    "# plot the DFT spectrogram\n",
    "S = np.abs(librosa.stft(linear_chirp, n_fft=dn_fft, \n",
    "                        hop_length=dhop_length))\n",
    "S_db = librosa.amplitude_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_db, n_fft=dn_fft, hop_length=dhop_length,\n",
    "                         sr=dsr, x_axis='time', y_axis='linear', \n",
    "                         ax=ax[0], cmap='jet')\n",
    "ax[0].set(title='DFT spectrogram')\n",
    "\n",
    "# plot the mel-scaled DFT spectrogram\n",
    "S = librosa.feature.melspectrogram(\n",
    "    y=linear_chirp, sr=dsr, n_fft=dn_fft, hop_length=dhop_length,\n",
    "    n_mels=128, fmax=dsr/2)\n",
    "S_db = librosa.power_to_db(S, ref=np.max)\n",
    "img=librosa.display.specshow(\n",
    "    S_db, n_fft=dn_fft, hop_length=dhop_length, x_axis='time', \n",
    "    y_axis='mel', sr=dsr, ax=ax[1], cmap='jet', fmax=dsr/2)\n",
    "ax[1].set(title='Mel-frequency spectrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501ce0e-d106-423c-b130-cd9b036d1f2a",
   "metadata": {},
   "source": [
    "Explain the differences and their origins between the two spectrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4cc7b3-cec8-40de-8245-4106411d86fa",
   "metadata": {},
   "source": [
    "## Liftering\n",
    "\n",
    "Now we're going to see how we lifter the DFT spectrogram.  It acts like a low pass filter and will produce a smoothed spectrogram without contamination from the fundamental frequency.  We'll plot and compared both spectrograms for the same <tt>assassiner</tt> utterance as earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16920f1-b71a-4e71-9e91-9e5492bf09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, sharey=True, figsize=(15, 3))\n",
    "\n",
    "# compute and plot the DFT spectrogram\n",
    "S = np.abs(librosa.stft(assassine, center=True, n_fft=dn_fft, \n",
    "                        hop_length=dhop_length))\n",
    "S_db = librosa.amplitude_to_db(S, ref=np.max)                                                                  \n",
    "librosa.display.specshow(\n",
    "    S_db, n_fft=dn_fft, hop_length=dhop_length, sr=dsr, \n",
    "    x_axis='time', y_axis='linear', ax=ax[0], cmap='jet')\n",
    "ax[0].set(title='DFT spectrogram')\n",
    "\n",
    "# compute the cepstrogram using the DCT\n",
    "C = scipy.fft.dct(np.log(S**2), axis=0)\n",
    "# do the liftering\n",
    "C = C[0:60,:]   # <-- change this to observe the effect\n",
    "                # with different numbers of cepstral coefficients\n",
    "    \n",
    "# now invert the DCT\n",
    "iC = scipy.fft.idct(C, n=256, axis=0)\n",
    "# and recover the magnitude\n",
    "S_hat = np.sqrt(np.exp(iC))\n",
    "\n",
    "# plot the liftered spectrogram\n",
    "S_hat_db = librosa.amplitude_to_db(S_hat, ref=np.max)                                                                  \n",
    "librosa.display.specshow(\n",
    "    S_hat_db, hop_length=dhop_length, sr=dsr, x_axis='time', \n",
    "    y_axis='linear', ax=ax[1], cmap='jet')\n",
    "ax[1].set(title='Liftered DFT spectrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f880b-21c8-403d-8403-9d01623a6200",
   "metadata": {},
   "source": [
    "Observe and comment on the effect with different liftering parameters, in other words fewer and fewer cepstral coefficients. At some point, the fundamental frequency harmonics will disappear. At what point (number of retained cepstral coefficients after litering) does this occur?  Can you explain this? \n",
    "\n",
    "To help us understand what is happenning here, we'll look at plots of the cepstral coefficients for a single frame of speech, and the corresponding spectrum before and after liftering.  As before, we'll initially select a frame at random.  You can also specify a specific interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21832a-20cf-413e-9611-62822efd5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = random.uniform(0, 1)*len(y)/sr \n",
    "# uncomment and change the following to whatever time instant for \n",
    "# which you want to plot the spectrum\n",
    "#t = 0.03\n",
    "print('Plotting spectrum at approximately', \n",
    "      \"{:.2f}\".format(t), 'seconds')\n",
    "\n",
    "# we'll use a slight larger dft window and hop size than the default\n",
    "frame_length=512\n",
    "hop_length=256\n",
    "\n",
    "# extract a frame at this time instant\n",
    "frames=librosa.util.frame(\n",
    "    assassine, frame_length=frame_length, hop_length=hop_length)\n",
    "frame_idx=int(t/(len(assassine)/sr)*frames.shape[1])\n",
    "frame_data=frames[:,frame_idx]\n",
    "print('Frame number', frame_idx, 'of', frames.shape[1])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(15, 10))\n",
    "x_tick_pos=[n for n in range(\n",
    "    0, frame_length //2 +1, frame_length // 16)]\n",
    "x_tick_lbl=[str(sr / frame_length * n) + 'Hz' for n in x_tick_pos]\n",
    "ax[1].set_xticks(x_tick_pos)\n",
    "ax[1].set_xticklabels(x_tick_lbl)\n",
    "ax[1].set_xlim([0, frame_length/2])\n",
    "\n",
    "# compute the dft-derived magnitude spectrum\n",
    "dft_spectrum=np.abs(scipy.fft.fft(frame_data, n=frame_length))\n",
    "ax[1].plot(dft_spectrum, label='DFT-derived spectrum')\n",
    "\n",
    "# compute cepstra using the DCT\n",
    "C = scipy.fft.dct(np.log(dft_spectrum**2))\n",
    "# do the liftering\n",
    "C_l = C[0:50]   # <-- change this to observe the effect with \n",
    "                # different numbers of cepstral coefficients\n",
    "ax[0].plot(C, label='Full cepstra')\n",
    "ax[0].plot(C_l, label='Liftered cepstra')\n",
    "ax[0].set(title='Cepstra', xlabel='Quefrency', ylabel='Magnitude')\n",
    "ax[0].legend()  # the second profile is just a truncated version\n",
    "                # of the first\n",
    "\n",
    "# now invert the DCT\n",
    "iC = scipy.fft.idct(C_l, n=frame_length)  # we still want to \n",
    "                                      # recover 256 components\n",
    "# recover and plot the magnitude\n",
    "dft_spectrum_hat = np.sqrt(np.exp(iC))\n",
    "ax[1].plot(dft_spectrum_hat, label='Liftered spectrum')\n",
    "ax[1].set(title='Spectrum', xlabel='Frequency', ylabel='Magnitude')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eecac9e-50b6-4e39-9bf2-31428815b5d7",
   "metadata": {},
   "source": [
    "Explain how cepstral coefficients are extracted and the importance of using a logorthim.  Plot the cepstra for an interval of voiced speech and estimate the fundamental frequency. For plots corresponding to intervals of voice speech, you should be able to identify a component that corresponds to the fundamental frequency.  You should then be able to set the number of retrained cepstral coefficients to either remove or retain it.  Determine the maximum possible number of cepstral coefficients for which the fundamental frequency is still *removed*.  Then, explain the characteristics which differentiate voiced and unvoiced sounds in the quefrency domain.\n",
    "\n",
    "Comment on the similarities and differences between DFT, LPC and liftered spectra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccfdb3a-277e-44a2-a6d5-282f65808026",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "We will now explore the correlation in Mel-scaled filterbank, linear predictive coding and cepstral coefficients across an entire utterance.  Try to understand the implications of correlation when features are used for modelling and classification/recognition.  We're going to extract 14 coefficients in each case, with a common framing configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894923df-c4b7-49be-9f5c-9b2a3f834c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same parameterisation for each feature\n",
    "frame_length=512\n",
    "hop_length=256\n",
    "n_coeffs=14\n",
    "\n",
    "# Mel-scaled filterbank coefficients\n",
    "mel_S = librosa.feature.melspectrogram(\n",
    "    y=assassine, sr=dsr, n_fft=frame_length, hop_length=hop_length,\n",
    "    n_mels=n_coeffs, fmax=dsr/2, center='False')\n",
    "\n",
    "# LPC coefficients\n",
    "[a, g, e] = lpc.lpcfit(assassine, n_coeffs, h=hop_length,\n",
    "                       w=hop_length)\n",
    "a=a[:,1:n_coeffs+1].T    # remove the extra a_0=1 coeff\n",
    "\n",
    "# Mel-scaled cepstral coefficients\n",
    "mfcc_S = librosa.feature.mfcc(\n",
    "    y=assassine, sr=dsr, n_fft=frame_length, \n",
    "    hop_length=hop_length, n_mfcc=n_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85662dc6-4783-4512-8bc9-4b9b7583aa8c",
   "metadata": {},
   "source": [
    "Now we'll estimate correlation matrices for each feature, plot them, and estimate the mean correlation coefficient in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad5b9e-0cbd-43a5-bff0-05e05c9a9e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the meshgrid\n",
    "X = np.arange(0, n_coeffs, 1)\n",
    "Y = np.arange(0, n_coeffs, 1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "# Plot the correlation matrices\n",
    "fig, ax = plt.subplots(ncols=3, subplot_kw={\"projection\": \"3d\"}, \n",
    "                       figsize=(20, 10))\n",
    "surf = ax[0].plot_surface(X, Y, numpy.corrcoef(mel_S), \n",
    "                          cmap='jet', linewidth=0, antialiased=False)\n",
    "surf = ax[1].plot_surface(X, Y, numpy.corrcoef(a), cmap='jet', \n",
    "                          linewidth=0, antialiased=False)\n",
    "surf = ax[2].plot_surface(X, Y, numpy.corrcoef(mfcc_S), cmap='jet', \n",
    "                          linewidth=0, antialiased=False)\n",
    "ax[0].set_xlim(0, 14);ax[1].set_xlim(0, 14);ax[2].set_xlim(0, 14)\n",
    "ax[0].set_ylim(0, 14);ax[1].set_ylim(0, 14);ax[2].set_ylim(0, 14)\n",
    "ax[0].view_init(25, 30); ax[1].view_init(25, 30); ax[2].view_init(25, 30)\n",
    "ax[0].set(title='Mel-scaled filterbank coefficients', \n",
    "          xlabel='coefficient #', ylabel='coefficient #')\n",
    "ax[1].set(title='LPC coefficients', xlabel='coefficient #', \n",
    "          ylabel='coefficient #')\n",
    "ax[2].set(title='MFCC coefficients',xlabel='coefficient #',\n",
    "          ylabel='coefficient #')\n",
    "\n",
    "# now calculate the mean correlation\n",
    "# for Mel-scaled filterbank coefficients\n",
    "xc=np.abs(np.corrcoef(mel_S)).mean()\n",
    "print(\n",
    "    'Mean correlation coefficient for Mel-scaled filterbank coefficients is ',\n",
    "    \"{:.4f}\".format(xc))\n",
    "# for LPC coefficients\n",
    "xc=np.abs(np.corrcoef(a)).mean()\n",
    "print(\n",
    "    'Mean correlation coefficient for LPC coefficients is                   ',\n",
    "    \"{:.4f}\".format(xc))\n",
    "# for MFCC coefficients\n",
    "xc=np.abs(np.corrcoef(mfcc_S)).mean()\n",
    "print(\n",
    "    'Mean correlation coefficient for Mel-scaled cepstral coefficients is   ',\n",
    "    \"{:.4f}\".format(xc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61c04a-7e5b-48c4-8c33-d48522d5b7eb",
   "metadata": {},
   "source": [
    "Explain which features, among the three choices, might be best for modelling and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408589a-c494-44c6-881b-9601a7dd9f7d",
   "metadata": {},
   "source": [
    "# Dynamic time warping (DTW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6deeb41-d459-47b8-9f78-ebda9d6e291a",
   "metadata": {},
   "source": [
    "We will use DTW to regenerate the same examples shown during your lectures using two renditions of the utterance '*cottage cheese with chives is delicious*', one shorter, one longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd90154-d01f-453b-b2ea-21237b9ec34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_long, sr1 = librosa.load('sm1_cln.wav', sr=dsr)\n",
    "y_short, sr2 = librosa.load('sm2_cln.wav', sr=dsr)\n",
    "\n",
    "# plot the time waveforms\n",
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(15, 3))\n",
    "\n",
    "librosa.display.waveshow(y_long, sr=sr1, ax=ax[0])\n",
    "ax[0].set(title='Longer utterance')\n",
    "ax[0].label_outer()\n",
    "\n",
    "librosa.display.waveshow(y_short, sr=sr2, ax=ax[1])\n",
    "ax[1].set(title='Shorter utterance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae287e9-ac63-4480-aa3e-aa77f98bda18",
   "metadata": {},
   "source": [
    "Listen to the longer utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ebc04-503f-406c-9b1c-f5bbdb25fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=y_long, rate=sr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2adc2-141c-481a-ad48-7e226bac2d74",
   "metadata": {},
   "source": [
    "And now the second, shorter utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc20ee-4928-4e70-8b97-a7d617148a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=y_short, rate=sr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c500d5-7edf-45f2-8eeb-9c791e6ad9a1",
   "metadata": {},
   "source": [
    "## Addition and linear warping\n",
    "If we compare these two utterances directly, with no alignment, then we stand to compare dissimilar speech units as can be perceived in the following example whereby the two sound files are simply added together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a21a1a-e280-4ea9-a7f9-1a91fa5373dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_long_pad = np.pad(y_long,(\n",
    "    0,max(y_long.size,y_short.size)-y_long.size),mode='constant')\n",
    "y_short_pad = np.pad(y_short,(\n",
    "    0,max(y_long.size,y_short.size)-y_short.size),mode='constant')\n",
    "y_sum = y_long_pad + y_short_pad\n",
    "IPython.display.Audio(data=y_sum, rate=sr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93e4b51-3701-41cf-b1c4-4b608a1fe1eb",
   "metadata": {},
   "source": [
    "Of course, even the duration of the two utterances is different.  We could resample the shorter utterance in linear fashion to align it with the longer utterance in terms of duration. We'll do this by resynthesising a linearly warped utterance from complex spectral estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b34b5e6-efc5-49e0-ab8d-bf7948c01fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the STFT of both utterances\n",
    "S_long = librosa.stft(y_long, n_fft=dn_fft, hop_length=dhop_length)\n",
    "S_short = librosa.stft(y_short, n_fft=dn_fft, hop_length=dhop_length)\n",
    "\n",
    "# determine the number of frames in the longest utterance\n",
    "l=max(S_long.shape[1], S_short.shape[1])\n",
    "\n",
    "# determine the frame indices that warp the shorter utterance \n",
    "# to the same length as the longer utterance - we'll do this\n",
    "# for both utterances in case you change/swap them\n",
    "i_short = np.zeros(l)\n",
    "i_long = np.zeros(l)\n",
    "for i in range(l):\n",
    "    i_short[i] = i/l*S_short.shape[1]\n",
    "    i_long[i] = i/l*S_long.shape[1]\n",
    "\n",
    "# resynthesize from the re-sampled STFT\n",
    "S_long_warp = librosa.phase_vocoder(S_long[:,i_long.astype(int)],\n",
    "                                    rate=1.0, hop_length=dhop_length)\n",
    "y_long_warp = librosa.istft(S_long_warp, hop_length=dhop_length)\n",
    "S_short_warp = librosa.phase_vocoder(S_short[:,i_short.astype(int)],\n",
    "                                     rate=1.0, hop_length=dhop_length)\n",
    "y_short_warp = librosa.istft(S_short_warp, hop_length=dhop_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928bdd5-0e5a-4b2a-946d-e301aa97efc8",
   "metadata": {},
   "source": [
    "Now we can listen again to the summed version of the two utterances, this time using the warped (stretched) shorter utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a6943-de64-4a4e-aefc-3b99cfee7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sum = y_long_warp + y_short_warp\n",
    "IPython.display.Audio(data=y_sum, rate=sr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051fbe6-a15a-4cca-96b4-1671a6f8ac48",
   "metadata": {},
   "source": [
    "While the utterances are aligned in terms of duration, we did not compensate for differences in the speaker rate and speech endpoints, for instance, meaning that we are still comparing dissimilar sounds.  We need to align the utterances so that we compare meaningfully-similar speech sounds.  We will now do this using DTW. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19816039-2759-4012-b212-e77fd95ce495",
   "metadata": {},
   "source": [
    "## Dynamic warping\n",
    "We'll use Mel-scaled frequency cepstral coefficients (MFCCs) as the representation but you could alter the code to use LPC coefficients, Mel-scaled filterbank outputs or even any other representation of your choosing. We will further use type 1 local continuity constraints and a cosine distance metric.  You can adjust the code easily to investigate the use of different constraints and distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b240425-3377-4b7a-8106-4112372c0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the mfcc coefficients for both utterances\n",
    "n_coeffs=14\n",
    "y_long_mfcc = librosa.feature.mfcc(\n",
    "    y=y_long, sr=sr1, n_fft=dn_fft, hop_length=dhop_length, \n",
    "    n_mfcc=n_coeffs)\n",
    "y_short_mfcc = librosa.feature.mfcc(\n",
    "    y=y_short, sr=sr2, n_fft=dn_fft, hop_length=dhop_length,\n",
    "    n_mfcc=n_coeffs)\n",
    "\n",
    "# type 1 local continuity constraints - you can change these to another type\n",
    "step_sizes_sigma = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "\n",
    "# create and plot the matrix of local distances\n",
    "d = scipy.spatial.distance.cdist(\n",
    "    y_long_mfcc.T, y_short_mfcc.T, metric='cosine')\n",
    "\n",
    "# compute the dynamic alignment using the the local distances\n",
    "D_A, wp = librosa.sequence.dtw(C=d, step_sizes_sigma=step_sizes_sigma)\n",
    "wp_s = librosa.frames_to_time(wp, sr=sr1, hop_length=dhop_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c80571-162d-41b2-9a94-ec7d29e6bd87",
   "metadata": {},
   "source": [
    "Matrices <tt>d</tt> and <tt>D_A</tt> are the matrix of local distances and accumulated distances/costs respectively. <tt>wp</tt> contains the warping paths for each utterance.  We can plot all three as in your lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbcebf-ee7b-426f-a848-3e98b72c20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, sharey=True, figsize=(15, 5))\n",
    "\n",
    "# plot the local distances\n",
    "img = librosa.display.specshow(\n",
    "    d, x_axis='time', y_axis='time', sr=sr1, cmap='gray_r', \n",
    "    hop_length=dhop_length, ax=ax[0])\n",
    "ax[0].set(title='Local distances $d$', xlabel='Time $(y_{short})$',\n",
    "          ylabel='Time $(y_{long})$')\n",
    "fig.colorbar(img, ax=ax[0]);\n",
    "\n",
    "# plot the accumulated distance matrix and the warping path\n",
    "img = librosa.display.specshow(\n",
    "    D_A, x_axis='time', y_axis='time', sr=sr1, cmap='gray_r', \n",
    "    hop_length=dhop_length, ax=ax[1])\n",
    "ax[1].plot(wp_s[:, 1], wp_s[:, 0], marker='o', color='r')\n",
    "ax[1].set(title='Accumulated distances $D_A$',\n",
    "       xlabel='Time $(y_{short})$', ylabel='Time $(y_{long})$')\n",
    "fig.colorbar(img, ax=ax[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278d4ea-a5d0-4824-96be-b820c105741c",
   "metadata": {},
   "source": [
    "Change the local continuity constratints and observe the differences in the optimal warping path.\n",
    "\n",
    "We'll now look at the dynamic nature of the warping path and see how it acts to ensure the comparison of similar speech units in deriving the global distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f9e8d-c5f8-4b33-a575-33476dbf1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(12, 6))\n",
    "\n",
    "# y_long\n",
    "librosa.display.waveshow(y_long, sr=sr1, ax=ax[0])\n",
    "ax[0].set(title='Longer utterance')\n",
    "ax[0].label_outer()\n",
    "\n",
    "# y_short\n",
    "librosa.display.waveshow(y_short, sr=sr2, ax=ax[1])\n",
    "ax[1].set(title='Shorter utterance')\n",
    "\n",
    "n_arrows = 20\n",
    "for tp1, tp2 in wp_s[::len(wp_s)//n_arrows]:\n",
    "    # Create a connection patch between the aligned time points in each subplot\n",
    "    con = ConnectionPatch(xyA=(tp1, 0), xyB=(tp2, 0),\n",
    "                          axesA=ax[0], axesB=ax[1],\n",
    "                          coordsA='data', coordsB='data',\n",
    "                          color='r', linestyle='--',\n",
    "                          alpha=0.5)\n",
    "    ax[1].add_artist(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3977e4-06f6-4bc7-bff4-5e7c7ab24ac7",
   "metadata": {},
   "source": [
    "The dashed red lines in the above plot show that DTW aligns the two utterances in a dynamic fashion, adjusting for differences in the inter-utterance speaking rates.  We can also plot the two warped utterances to check the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe56b82-1cbf-44b4-9ccd-6af92f07aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resynthesize from the re-sampled STFT now according to the \n",
    "# non-linear, dynamic warping paths\n",
    "S_long_warp = librosa.phase_vocoder(\n",
    "    S_long[:,numpy.flip(wp[:,0])], rate=1.0, hop_length=dhop_length)\n",
    "y_long_warp = librosa.istft(S_long_warp, hop_length=dhop_length)\n",
    "S_short_warp = librosa.phase_vocoder(\n",
    "    S_short[:,numpy.flip(wp[:,1])], rate=1.0, hop_length=dhop_length)\n",
    "y_short_warp = librosa.istft(S_short_warp, hop_length=dhop_length)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(12, 6))\n",
    "\n",
    "# y_long\n",
    "librosa.display.waveshow(y_long_warp, sr=sr2, ax=ax[0])\n",
    "ax[0].set(title='Longer utterance')\n",
    "\n",
    "# y_short\n",
    "librosa.display.waveshow(y_short_warp, sr=sr2, ax=ax[1])\n",
    "ax[1].set(title='Shorter utterance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd430b-1b0d-416e-b4bd-4ee4f5da1035",
   "metadata": {},
   "source": [
    "Now we see that both utterances are at least visually aligned in the time series.  We'll now listen to the sum of the two warped utterances as a final check that the alignment is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d974ff2-90f1-4b4e-ad3b-2d5939769b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sum = y_long_warp + y_short_warp\n",
    "IPython.display.Audio(data=y_sum, rate=sr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe285e42-8645-42a5-8541-1bd416ef28f8",
   "metadata": {},
   "source": [
    "If the alignment is effective, you shoud almost hear only a single utterance. \n",
    "\n",
    "Last, we can look at the accumlated distance under DTW.  It is not important here, but it is the distance/cost we would use if we were performing classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1712ffa-de4c-4883-b81e-05cbe8a9fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total accumulated distance/cost is', \"{:.4f}\".format(D_A[-1,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc64fa-3236-411e-9a37-2cdb3db65e52",
   "metadata": {},
   "source": [
    "# Hidden Markov modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f59a3-c854-49ea-844e-a58e806dcad2",
   "metadata": {},
   "source": [
    "We're going to study the very simple example that we studied in your lectures, namely a trivial speech activity detection problem.  We'll use the <tt>hmmlearn</tt> library to do this. It implements solutions to the three, core HMM problems we've studied and you should be able to use it to reproduce the same or similar results that you derived yourselves, so long as you initialise the HMM with the same parameters $\\lambda=(\\pi, A, B)$.\n",
    "\n",
    "Let's set up the initial HMM parameters for a two-state HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca06a0-b92e-4543-8b4f-da7297359364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and name the states\n",
    "states = [\"non-speech\", \"speech\"]\n",
    "n_states = len(states)\n",
    "\n",
    "# define and name the observations\n",
    "observations = [\"A\", \"B\", \"C\"]\n",
    "n_observations = len(observations)\n",
    "\n",
    "# define the discrete HMM - n_components is the number of states\n",
    "model = hmm.MultinomialHMM(\n",
    "    n_components=n_states, verbose=True, algorithm='viterbi',\n",
    "    n_iter=1000, params='ste', init_params='', random_state=14)\n",
    "\n",
    "# starting state probabilities\n",
    "pi = np.array([1.0, 0.0])\n",
    "model.startprob_ = pi\n",
    "\n",
    "# transition probabilities\n",
    "A = np.array([[0.7, 0.3],\n",
    "              [0.3, 0.7]])\n",
    "model.transmat_ = A\n",
    "\n",
    "# emission probabilities \n",
    "B = np.array([[0.7, 0.1, 0.2],\n",
    "              [0.1, 0.5, 0.4]])\n",
    "model.emissionprob_ = B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19105bf-6486-498f-a06d-f322980381c3",
   "metadata": {},
   "source": [
    "The HMM is now defined and initialised with some ad-hoc parameters. We will now suppose an observation sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48561d-fdcc-4b2e-8f68-ac79ebed7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an observation sequence\n",
    "X = np.atleast_2d([0,1,2,0]).T\n",
    "print(\"Observations:\", \", \".join(map(lambda x: observations[int(x)], X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc3fc4-9cb2-43c1-a42f-6f5bda1dfa3f",
   "metadata": {},
   "source": [
    "Given this observation sequence, we'll now compute the most probable state sequence and the corresponding probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c6e2c-79a6-4fda-9aec-a5a3446f5a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode - this is the probability of the state sequence, so Viterbi.  \n",
    "# It is not the forward algorithm\n",
    "X_hat = model.decode(X)\n",
    "\n",
    "# now determine the state labels to which this sequence corresponds\n",
    "print(\"State sequence:\", \", \".join(map(lambda x: states[int(x)], X_hat[1])))\n",
    "print(\"Log probability of the produced state sequence:\", \n",
    "      \"{:.2f}\".format(X_hat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99169e2-49eb-43c4-8ac8-e6b37e7fe950",
   "metadata": {},
   "source": [
    "The log probability is $P(O,q|\\lambda)$.  Notice the difference between this and the probability of this sequence given the model $P(O|\\lambda)$ as computed using the forward sequence.  We can estimate that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940c5f1-0b60-42ac-9964-4b9f49967c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_score=model.score(X)\n",
    "print(\"Log probability under the original model:\", \n",
    "      \"{:.2f}\".format(original_model_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f50a8-875d-4b2d-9bda-d6bda5696ed3",
   "metadata": {},
   "source": [
    "In the example we studied in class, we assumed the precise, known state sequence and estimated a new model based upon this assumed sequence.  In practice, we do not *know* the exact sequence and would adjust the model while taking into account every possible state sequence.  We can do this by *fitting* the model to the observations.  For this trivial example, the model should converge to the same model we produced in the lecture example.\n",
    "\n",
    "Let's fit a new model (initialised with the original configuration) to the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a13cb4-0287-4d08-bdbc-6ac60b34eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the new HMM\n",
    "new_model = hmm.MultinomialHMM(\n",
    "    n_components=n_states, verbose=True, algorithm='viterbi',\n",
    "    n_iter=10, params='ste', init_params='', random_state=14)\n",
    "new_model.startprob_ = pi\n",
    "new_model.transmat_ = A\n",
    "new_model.emissionprob_ = B\n",
    "new_model.fit(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762007a-b043-492a-8f89-1dc7683435e6",
   "metadata": {},
   "source": [
    "And now let's see the resulting, updated model paramters.  Here are the new $\\pi$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff6e05-f36f-4a51-90a2-34232cee273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_model.startprob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d8e3c-5b0d-42c1-89e3-4d471a898485",
   "metadata": {},
   "source": [
    "Which is the same as before.  This is to be expected since we only have a single observation sequence and this sequence starts in the non-speech state.  Now for the updated transition matrix $A.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88edc444-a14b-474e-9acc-4b0383e1d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_model.transmat_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8edd86-4a2e-4406-8ce2-bb725897444b",
   "metadata": {},
   "source": [
    "This is very close to the example in lectures in which there are no transitions from the non-speech to non-speech state, hence the probability of this transition is almost zero.  Last, here's the emission probability matrix, $B.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98120d-06b1-4b1c-ba4e-a68788ae992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_model.emissionprob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d295d7-86b7-46bc-b70a-43776afe9d36",
   "metadata": {},
   "source": [
    "Which is again close to the result for the lecture example.  Last, let's check that $P(O|\\lambda)$ is now higher for the new model than it was for the orignal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c18b19-40d5-4df0-aefd-7c2e2bdce9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Log probability under the original model:\", \n",
    "      \"{:.2f}\".format(original_model_score))\n",
    "new_model_score=new_model.score(X)\n",
    "print(\"Log probability under the new model:     \", \n",
    "      \"{:.2f}\".format(new_model_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34125ad4-61ed-4512-9d0a-233e01e498b1",
   "metadata": {},
   "source": [
    "Which is indeed higher, showing that our model has converged to a configuration which (locally) maximises $P(O|\\lambda)$ for the given training data.\n",
    "\n",
    "**Briefly** describe how you would design an automatic speech recognition system to distinguish between \"On\" and \"Off\" utterances using HMMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
